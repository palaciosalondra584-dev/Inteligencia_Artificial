# -*- coding: utf-8 -*-
"""Función OR/ Perceptron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lcdIykwpscEilDiZ2c1typzKM2HyozKG

**¿Que hace la neurona?**


La neurona se activa (1) si al menos una de sus entradas es verdadera (1) y
no se activa (0) solo si todas sus entradas son falsas (0).
La neurona aprende los pesos y el sesgo necesarios hasta que se active.
"""

#Importamos la libreria numpy
import numpy as np

# Definición de la función de activación (escalón unitario)
# Esta función devuelve 1 si la entrada es mayor o igual a 0, y 0 en caso contrario.
def activacion(suma_ponderada):
    return 1 if suma_ponderada >= 0 else 0

# Datos de entrenamiento para la función OR
# Cada fila representa un ejemplo: [entrada1, entrada2, salida_esperada]
# (0, 0) -> 0
# (0, 1) -> 1
# (1, 0) -> 1
# (1, 1) -> 1
datos_entrenamiento = np.array([
    [0, 0, 0],
    [0, 1, 1],
    [1, 0, 1],
    [1, 1, 1]
])

# Inicialización de los pesos y el sesgo
# Se inicializan con valores pequeños aleatorios
pesos = np.random.rand(2) # Dos pesos, uno para cada entrada
sesgo = np.random.rand(1) # Un sesgo

# Tasa de aprendizaje
# Determina qué tan rápido se ajustan los pesos y el sesgo durante el entrenamiento
tasa_aprendizaje = 0.1

# Número de épocas (iteraciones de entrenamiento)
# Cuántas veces se recorrerán todos los datos de entrenamiento
epocas = 100

# Entrenamiento del perceptrón
for epoca in range(epocas):
    # Iterar sobre cada ejemplo en los datos de entrenamiento
    for ejemplo in datos_entrenamiento:
        entrada = ejemplo[:2] # Las dos primeras columnas son las entradas
        salida_esperada = ejemplo[2] # La última columna es la salida esperada

        # Calcular la suma ponderada de las entradas y los pesos, más el sesgo
        # (entrada1 * peso1) + (entrada2 * peso2) + sesgo
        suma_ponderada = np.dot(entrada, pesos) + sesgo

        # Aplicar la función de activación para obtener la salida predicha por el perceptrón
        salida_predicha = activacion(suma_ponderada)

        # Calcular el error
        # La diferencia entre la salida esperada y la salida predicha
        error = salida_esperada - salida_predicha

        # Actualizar los pesos y el sesgo si hay un error
        # Se ajustan en proporción al error, la tasa de aprendizaje y la entrada
        pesos += tasa_aprendizaje * error * entrada
        sesgo += tasa_aprendizaje * error

print(f"Pesos finales: {pesos}")
print(f"Sesgo final: {sesgo}")

# Prueba del perceptrón entrenado
print("\nProbando el perceptrón:")
for ejemplo in datos_entrenamiento:
    entrada = ejemplo[:2]
    salida_esperada = ejemplo[2]

    # Calcular la suma ponderada con los pesos y sesgo entrenados
    suma_ponderada = np.dot(entrada, pesos) + sesgo

    # Obtener la salida predicha
    salida_predicha = activacion(suma_ponderada)

    # Imprimir la entrada, la salida esperada y la salida predicha
    print(f"Entrada: {entrada}, Salida esperada: {salida_esperada}, Salida predicha: {salida_predicha}")